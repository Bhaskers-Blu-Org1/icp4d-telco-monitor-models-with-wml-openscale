{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor your ML Models using Watson OpenScale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install the necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watson OpenScale Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ibm-ai-openscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn version 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==0.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watson Machine Learning Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install watson-machine-learning-client -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the Notebook after Installing the required packages. By clicking on `Kernel>Restart`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm, metrics\n",
    "from scipy import sparse\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, Imputer, OneHotEncoder\n",
    "import json\n",
    "import ibm_db\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from ibm_ai_openscale import APIClient4ICP\n",
    "from ibm_ai_openscale.engines import *\n",
    "from ibm_ai_openscale.utils import *\n",
    "from ibm_ai_openscale.supporting_classes import PayloadRecord, Feature\n",
    "from ibm_ai_openscale.supporting_classes.enums import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"AIOS_Model\"\n",
    "DEPLOYMENT_NAME=\"AIOS_Deployment\"\n",
    "# Ensure you create a an empty Schema and store the name in this variable\n",
    "SCHEMA_NAME=\"DB2INST1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Add Dataset\n",
    "\n",
    "Select the `Insert Pandas Dataframe` option, after selecting the below cell. Ensure the variable name is `df_data_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Add your WML Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WML_CREDENTIALS ={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Update your AIOS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WOS_CREDENTIALS={\n",
    "    \"url\": \"<instance-url>\",\n",
    "    \"username\": \"<username-of-CP4D-instance>\",\n",
    "    \"password\": \"<password-of-CP4D-instance>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Add your Db credentials\n",
    "\n",
    "#### These Db credentials are needed ONLY if you have NOT configured your `OpenScale Datamart`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATABASE_CREDENTIALS = {\n",
    "    \"hostname\": \"DB-Server-IP\",\n",
    "    \"username\": \"DB-username\",\n",
    "    \"password\": \"DB-Pwd\",\n",
    "    \"port\": xxxx,\n",
    "    \"db\": \"DB-name\",\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Call Drop Model using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_data_1.drop(['Call_Drop_Actual'], axis=1)\n",
    "y=df_data_1.loc[:, 'Call_Drop_Actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Add a categorical transformer to your model pipeline. \n",
    "    You will need to add a label encoder into the model pipeline before storing it into WML '''\n",
    "\n",
    "categorical_features = [\"Start_Time_MM_DD_YYYY\", \"Traffic\", \" _conds\", \"Start_Time_HH_MM_SS_s\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', svm.SVC(kernel='linear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the model on WML\n",
    "published_model = client.repository.store_model(model=model, meta_props=MODEL_NAME, \\\n",
    "                                                training_data=X_train, training_target=y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "\n",
    "published_model_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Deployment for your stored model\n",
    "\n",
    "created_deployment = client.deployments.create(published_model_uid, DEPLOYMENT_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scoring_endpoint = None\n",
    "deployment_uid=created_deployment['metadata']['guid']\n",
    "print(deployment_uid)\n",
    "\n",
    "for deployment in client.deployments.get_details()['resources']:\n",
    "    if deployment_uid in deployment['metadata']['guid']:\n",
    "        scoring_endpoint = deployment['entity']['scoring_url']\n",
    "        \n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup your Watson Openscale Dashboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create the Watson Openscale Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_client = APIClient4ICP(aios_credentials=WOS_CREDENTIALS)\n",
    "ai_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Add your Machine Learning Provider\n",
    "\n",
    "If you have already bound the ML Provider to the Openscale instance, then just retrieve the binding_uid, by commenting first line and uncommenting the second line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binding_uid = ai_client.data_mart.bindings.add('WML instance', WatsonMachineLearningInstance(WML_CREDENTIALS))\n",
    "#bindings_details = ai_client.data_mart.bindings.get_details()\n",
    "ai_client.data_mart.bindings.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_client.data_mart.bindings.list_assets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Setup the Datamart on AI OpenScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_mart_details = ai_client.data_mart.get_details()\n",
    "    print('Using existing external datamart')\n",
    "except:\n",
    "    print('Setting up external datamart')\n",
    "    ai_client.data_mart.setup(db_credentials=DATABASE_CREDENTIALS, schema=SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Perform Initial Scoring for your Model Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=X_test.tail(20)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data=list(list(x) for x in zip(*(score[x].values.tolist() for x in score.columns)))\n",
    "scoring_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=list(X_test.columns)\n",
    "print(len(fields))\n",
    "fields, scoring_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data = {\n",
    "    \"fields\": fields,\n",
    "    \"values\": scoring_data\n",
    "  }\n",
    "request_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_scoring = {\"fields\": fields,\"values\": scoring_data}\n",
    "scoring_response = client.deployments.score(scoring_endpoint, payload_scoring)\n",
    "\n",
    "print(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Create a new Subscription "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription = ai_client.data_mart.subscriptions.add(WatsonMachineLearningAsset(\n",
    "    published_model_uid,\n",
    "    problem_type=ProblemType.BINARY_CLASSIFICATION,\n",
    "    input_data_type=InputDataType.STRUCTURED,\n",
    "    label_column='Call_Drop_Actual',\n",
    "    prediction_column='prediction',\n",
    "    probability_column='prediction_probability',\n",
    "    categorical_columns=[\"Start_Time_MM_DD_YYYY\",\"Start_Time_HH_MM_SS_s\",\" _conds\",\"Traffic\"],\n",
    "    feature_columns = [\"outgoing_site_id\",\"Start_Time_MM_DD_YYYY\",\"Start_Time_HH_MM_SS_s\",\"Call_Service_Duration\",\" _conds\",\" _dewptm\",\" _fog\",\" _hail\",\" _hum\",\" _pressurem\",\"total number_of_calls\",\"total call duration (min)\",\"Traffic\",\"lat\",\"long\",\"Call_Drop_Count\",\"Total_Calls\",\"Call_Drop_Perc\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriptions_uids = ai_client.data_mart.subscriptions.get_uids()\n",
    "ai_client.data_mart.subscriptions.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Perform Inital Payload Logging\n",
    "Note: You may re-use this code snippet by modifying the request_data variable to perform payload logging after finishing the initial dashboard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From the output of the above table choose your model name and copy the uid against it. Store the uid in the subscription_uid variable\n",
    "\n",
    "\n",
    "subscription_uid=\"497a4336-6142-4f41-b00c-249f48776a41\"\n",
    "from ibm_ai_openscale import APIClient4ICP\n",
    "from ibm_ai_openscale.supporting_classes import PayloadRecord\n",
    "\n",
    "\n",
    "subscription = ai_client.data_mart.subscriptions.get(subscription_uid=subscription_uid)\n",
    "\n",
    "\"\"\"\n",
    "request_data - input to scoring endpoint in supported by Watson OpenScale format\n",
    "response_data - output from scored model in supported by Watson OpenScale format\n",
    "response_time - scoring request response time [ms] (integer type)\n",
    "\n",
    "Example:\n",
    "\n",
    "request_data = {\n",
    "    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\"],\n",
    "    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026]]\n",
    "  }\n",
    "\n",
    "response_data = {\n",
    "    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\", \"probability\", \"prediction\", \"DRUG\"],\n",
    "    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026, [0.82, 0.07, 0.0, 0.05, 0.03], 0.0, \"drugY\"]]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "records = [PayloadRecord(request=request_data, response=scoring_response, response_time=18), \n",
    "                PayloadRecord(request=request_data, response=scoring_response, response_time=12)]\n",
    "\n",
    "subscription.payload_logging.store(records=records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Setup Quality Monitoring\n",
    "\n",
    "```NOTE: If you are using the dataset provided in the dashboard, leave the threshold monitors to these values. However, if you are using your own dataset, you can play around with the threshold value (value b/w 0 and 1) according to your requirement.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "subscription.quality_monitoring.enable(threshold=0.95, min_records=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Log Feedback Data to your Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_data_raw=pd.concat([X_test,y_test],axis=1)\n",
    "feedback_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_data=feedback_data_raw.tail(20).values.tolist()\n",
    "feedback_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_scoring={\n",
    "    \"data\":feedback_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subscription.feedback_logging.store(feedback_scoring['data'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription.feedback_logging.show_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run an inital quality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_details = subscription.quality_monitoring.run(background_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription.quality_monitoring.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "quality_pd = subscription.quality_monitoring.get_table_content(format='pandas')\n",
    "quality_pd.plot.barh(x='id', y='value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Setup the Fairness Monitors\n",
    "\n",
    "The code below configures fairness monitoring for our model. It turns on monitoring for two features, _conds(Weather Condition) and Traffic for the cell tower. In each case, we must specify:\n",
    "  * Which model feature to monitor\n",
    "  * One or more **majority** groups, which are values of that feature that we expect to receive a higher percentage of favorable outcomes\n",
    "  * One or more **minority** groups, which are values of that feature that we expect to receive a higher percentage of unfavorable outcomes\n",
    "  * The threshold at which we would like OpenScale to display an alert if the fairness measurement falls below (in this case, 95%)\n",
    "\n",
    "Additionally, we must specify which outcomes from the model are favourable outcomes, and which are unfavourable. We must also provide the number of records OpenScale will use to calculate the fairness score. In this case, OpenScale's fairness monitor will run hourly, but will not calculate a new fairness rating until at least 5 records have been added. Finally, to calculate fairness, OpenScale must perform some calculations on the training data, so we provide the dataframe containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription.fairness_monitoring.enable(\n",
    "            features=[\n",
    "                Feature(\"Traffic\", majority=['Low'], minority=['High','Medium'], threshold=0.95),\n",
    "                Feature(\" _conds\", majority=['Haze','Rain'], minority=['Clear','Fog','Partly Cloudy'], threshold=0.95)\n",
    "            ],\n",
    "            favourable_classes=[1],\n",
    "            unfavourable_classes=[0],\n",
    "            min_records=5,\n",
    "            training_data=df_data_1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "\n",
    "run_details = subscription.fairness_monitoring.run(background_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "\n",
    "subscription.fairness_monitoring.show_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add some more Payload (Optional for populating your dashboard)\n",
    "\n",
    "If you wish to add some Payload Data. Take different sections of your test dataset and send to OpenScale as shown below-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=X_test.head(100)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data=list(list(x) for x in zip(*(score[x].values.tolist() for x in score.columns)))\n",
    "scoring_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=list(X_test.columns)\n",
    "print(len(fields))\n",
    "fields, scoring_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data = {\n",
    "    \"fields\": fields,\n",
    "    \"values\": scoring_data\n",
    "  }\n",
    "request_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From the output of the above table choose your model name and copy the uid against it. Store the uid in the subscription_uid variable\n",
    "\n",
    "\n",
    "\n",
    "from ibm_ai_openscale import APIClient4ICP\n",
    "from ibm_ai_openscale.supporting_classes import PayloadRecord\n",
    "\n",
    "\n",
    "subscription = ai_client.data_mart.subscriptions.get(subscription_uid=subscription_uid)\n",
    "\n",
    "\"\"\"\n",
    "request_data - input to scoring endpoint in supported by Watson OpenScale format\n",
    "response_data - output from scored model in supported by Watson OpenScale format\n",
    "response_time - scoring request response time [ms] (integer type)\n",
    "\n",
    "Example:\n",
    "\n",
    "request_data = {\n",
    "    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\"],\n",
    "    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026]]\n",
    "  }\n",
    "\n",
    "response_data = {\n",
    "    \"fields\": [\"AGE\", \"SEX\", \"BP\", \"CHOLESTEROL\", \"NA\", \"K\", \"probability\", \"prediction\", \"DRUG\"],\n",
    "    \"values\": [[28, \"F\", \"LOW\", \"HIGH\", 0.61, 0.026, [0.82, 0.07, 0.0, 0.05, 0.03], 0.0, \"drugY\"]]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "records = [PayloadRecord(request=request_data, response=scoring_response, response_time=18), \n",
    "                PayloadRecord(request=request_data, response=scoring_response, response_time=12)]\n",
    "\n",
    "subscription.payload_logging.store(records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
